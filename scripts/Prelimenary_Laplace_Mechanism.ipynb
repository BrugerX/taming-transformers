{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Steps in this notebook:\n",
    "\n",
    "#### Step 1: Load the model and the image\n",
    "\n",
    "### Step 2: Create the Laplace mechanism function by modifying the original ML model's way of encoding the image\n",
    "\n",
    "### Step 3: Plot different images (Original vs Encoded vs Encoded w. Noise) for CelebAHQ and PublicAHQ"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from PIL import Image,ImageShow\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from omegaconf import OmegaConf\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import main\n",
    "import taming.modules.losses.vqperceptual\n",
    "from taming.models.cond_transformer import Net2NetTransformer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Get the dependencies"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 einops transformers\n",
    "\n",
    "sys.path.append(\".\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prepare configurations of the CelebAHQ model as well as checkpoints."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Prepare CelebAHQ configurations\n",
    "config_path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\configs\\faceshq_transformer.yaml\"\n",
    "celebAHQ_config = OmegaConf.load(config_path)\n",
    "print(yaml.dump(OmegaConf.to_container(celebAHQ_config)))\n",
    "\n",
    "#Init model with the chosen architecture and configurations\n",
    "model = Net2NetTransformer(**celebAHQ_config.model.params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Load checkpoints\n",
    "ckpt_path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\configs\\faceshq.ckpt\"\n",
    "sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "model.load_state_dict(sd)\n",
    "missing, unexpected = model.load_state_dict(sd, strict=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Put model in evaluation mode\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#We look at the codebook to check whether or not it ressembles a normal distribution around 0.\n",
    "\n",
    "norms = []\n",
    "\n",
    "#Uncomment the code below to get a graph of how the freshly initialized codebook should look.\n",
    "#model.first_stage_model.quantize.embedding.weight.data.uniform_(-1.0/(model.first_stage_model.quantize.n_e),1/(model.first_stage_model.quantize.n_e))\n",
    "\n",
    "for codebook in model.first_stage_model.quantize.parameters():\n",
    "    codebook\n",
    "    for zq in codebook:\n",
    "        norms += [np.linalg.norm(zq)]\n",
    "\n",
    "#We know for a fact, that all norms will be equal to or less than 2 if it has just been instantiated\n",
    "assert(len([norm for norm in norms if norm>2])>0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load example data\n",
    "\n",
    "Load an example segmentation and visualize."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the function to load an image as a tensor\n",
    "def load_image_tensor(path):\n",
    "    image = Image.open(path)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.PILToTensor()\n",
    "    ])\n",
    "    img_tensor = transform(image).permute(1,2,0).float()/255\n",
    "    return img_tensor.permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "# Load the images\n",
    "AHQ_example_path = r\"C:\\Users\\DripTooHard\\Pictures\\DivVisaApp.jpg\"\n",
    "FFHQ_example_path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\data\\FFHQ\\00000\\00001.png\"\n",
    "\n",
    "AHQ_image = load_image_tensor(AHQ_example_path)\n",
    "FFHQ_image = load_image_tensor(FFHQ_example_path)\n",
    "\n",
    "# Plot the images with titles\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(AHQ_image[0].permute(1, 2, 0))\n",
    "plt.title('CelebAHQ')\n",
    "plt.axis('off')  # Hide the axis\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(FFHQ_image[0].permute(1, 2, 0))\n",
    "plt.title('FFHQ')\n",
    "plt.axis('off')  # Hide the axis\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 2: We now confirm, that it can correctly reconstruct the images.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def deconstruct_reconstruct(image):\n",
    "    image = image.type(torch.FloatTensor)  # Ensure the image tensor is of type FloatTensor\n",
    "    image_zq, image_z_indices = model.encode_to_z(image)\n",
    "    image_recon = model.decode_to_img(image_z_indices, image_zq.shape)\n",
    "    return image_recon\n",
    "\n",
    "model.set_epsilon(0.9)\n",
    "recon = deconstruct_reconstruct(AHQ_image)\n",
    "recon = recon"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the images with titles\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(recon[0].float().permute(1, 2, 0))\n",
    "plt.title('CelebAHQ')\n",
    "plt.axis('off')  # Hide the axis\n",
    "plt.savefig(\"TestImageReconstructed\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Step 3:\n",
    "\n",
    "We now add the ability to use the laplace mechanism when encoding the images.\n",
    "\n",
    "We now recreate the images with noise and plot them next to eachother.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "\n",
    "def deconstruct_reconstruct(image, epsilon):\n",
    "    image = image.type(torch.FloatTensor)  # Ensure the image tensor is of type FloatTensor\n",
    "    model.set_epsilon(epsilon)\n",
    "    image_zq, image_z_indices = model.encode_to_z(image)\n",
    "    image_recon = model.decode_to_img(image_z_indices, image_zq.shape)\n",
    "    return image_recon\n",
    "\n",
    "#Ignore this for now\n",
    "def deconstruct_reconstruct_mix(image1,image2,epsilon):\n",
    "    image1 = image1.type(torch.FloatTensor)  # Ensure the image tensor is of type FloatTensor\n",
    "    image2 = image2.type(torch.FloatTensor)  # Ensure the image tensor is of type FloatTensor\n",
    "    image_zq, image_z_indices = model.encode_to_z_mix(image1,image2, epsilon)\n",
    "    image_recon = model.decode_to_img(image_z_indices, image_zq.shape)\n",
    "    return image_recon\n",
    "\n",
    "\n",
    "step_size = 0.01\n",
    "max_range = 1\n",
    "\n",
    "epsilon_values = np.arange(0,max_range,step_size)\n",
    "\n",
    "folder = fr\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\ExperimentResults\\NoisyReconstructions\\Laplace{max_range}{step_size}\"\n",
    "try:\n",
    "    os.makedirs(folder)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if(True):\n",
    "    for epsilon in epsilon_values:\n",
    "        # Assuming AHQ_image and FFHQ_image are defined and are tensors\n",
    "        AHQ_recon = deconstruct_reconstruct(AHQ_image, epsilon)\n",
    "        FFHQ_recon = deconstruct_reconstruct(FFHQ_image, epsilon)\n",
    "\n",
    "        # Plot the images with titles\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(AHQ_recon[0].permute(1, 2, 0))\n",
    "        plt.title('CelebAHQ-Reconstructed')\n",
    "        plt.axis('off')  # Hide the axis\n",
    "\n",
    "        # Use numpy to clip the values and ensure the correct data type for imsave\n",
    "\n",
    "        plt.imsave(f\"{folder}\\AHQ\\{epsilon}CelebAHQ-Reconstructed.png\", AHQ_recon[0].permute(1, 2, 0).numpy().clip(0, 1))\n",
    "        plt.close()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(FFHQ_recon[0].permute(1, 2, 0))\n",
    "        plt.title('FFHQ-reconstructed')\n",
    "        plt.axis('off')  # Hide the axis\n",
    "\n",
    "        # Use numpy to clip the values and ensure the correct data type for imsave\n",
    "        plt.imsave(f\"{folder}\\FFHQ\\{epsilon}FFHQ-reconstructed.png\", FFHQ_recon[0].permute(1, 2, 0).numpy().clip(0, 1))\n",
    "        plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Step 4 Dataloader Object\n",
    "\n",
    "The point is now to create an object, which either:\n",
    "\n",
    "1) takes a 3-tuple of percentages and a .txt file of image paths and seperates the images paths into three different .txt files, one for validation, one for testing and a third for training.\n",
    "\n",
    "2) A tuple with (valid_paths.txt,test_paths.txt,train_paths.txt)\n",
    "\n",
    "We would then like for the dataloader object to be able to divide the image paths into minibatches, which we should be able to get by using .getMinibatch(i) = ith minibatch. It should have a parameter, which based on the size of the minibatches, tells us how many minibatches there are per epoch.\n",
    "\n",
    "Finally, we would like for it to be able to turn each minibatch of paths into a minibatch of images with getMinibatchImages(i).\n",
    "\n",
    "The dimension of the images should be specified beforehand, and should match the dimensions of our VQGAN model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}