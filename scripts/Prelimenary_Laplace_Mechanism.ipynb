{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Steps in this notebook:\n",
    "\n",
    "#### Step 1: Load the model and the image\n",
    "\n",
    "### Step 2: Create the Laplace mechanism function by modifying the original ML model's way of encoding the image\n",
    "\n",
    "### Step 3: Plot different images (Original vs Encoded vs Encoded w. Noise) for CelebAHQ and PublicAHQ"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from PIL import Image,ImageShow\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from omegaconf import OmegaConf\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import main\n",
    "import taming.modules.losses.vqperceptual\n",
    "from taming.models.cond_transformer import Net2NetTransformer\n",
    "\n",
    "#We don't want to run through a bunch of epsilons.\n",
    "doGradualEpsilonXperiment = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Get the dependencies"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 einops transformers\n",
    "\n",
    "sys.path.append(\".\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prepare configurations of the CelebAHQ model as well as checkpoints."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "  params:\n",
      "    batch_size: 2\n",
      "    num_workers: 8\n",
      "    train:\n",
      "      params:\n",
      "        coord: true\n",
      "        crop_size: 256\n",
      "        size: 256\n",
      "      target: taming.data.faceshq.FacesHQTrain\n",
      "    validation:\n",
      "      params:\n",
      "        coord: true\n",
      "        crop_size: 256\n",
      "        size: 256\n",
      "      target: taming.data.faceshq.FacesHQValidation\n",
      "  target: main.DataModuleFromConfig\n",
      "model:\n",
      "  base_learning_rate: 4.5e-06\n",
      "  params:\n",
      "    cond_stage_config:\n",
      "      params:\n",
      "        down_factor: 16\n",
      "        n_embed: 1024\n",
      "      target: taming.modules.misc.coord.CoordStage\n",
      "    cond_stage_key: coord\n",
      "    first_stage_config:\n",
      "      params:\n",
      "        ckpt_path: C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\configs\\faceshq.ckpt\n",
      "        ddconfig:\n",
      "          attn_resolutions:\n",
      "          - 16\n",
      "          ch: 128\n",
      "          ch_mult:\n",
      "          - 1\n",
      "          - 1\n",
      "          - 2\n",
      "          - 2\n",
      "          - 4\n",
      "          double_z: false\n",
      "          dropout: 0.0\n",
      "          in_channels: 3\n",
      "          num_res_blocks: 2\n",
      "          out_ch: 3\n",
      "          resolution: 256\n",
      "          z_channels: 256\n",
      "        embed_dim: 256\n",
      "        lossconfig:\n",
      "          target: taming.modules.losses.DummyLoss\n",
      "        n_embed: 1024\n",
      "      target: taming.models.vqgan.LAPVQ\n",
      "    transformer_config:\n",
      "      params:\n",
      "        block_size: 512\n",
      "        n_embd: 1024\n",
      "        n_head: 16\n",
      "        n_layer: 24\n",
      "        vocab_size: 1024\n",
      "      target: taming.modules.transformer.mingpt.GPT\n",
      "  target: taming.models.cond_transformer.Net2NetTransformer\n",
      "\n",
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n"
     ]
    }
   ],
   "source": [
    "#Prepare CelebAHQ configurations\n",
    "config_path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\configs\\faceshq_transformer.yaml\"\n",
    "celebAHQ_config = OmegaConf.load(config_path)\n",
    "print(yaml.dump(OmegaConf.to_container(celebAHQ_config)))\n",
    "\n",
    "#Init model with the chosen architecture and configurations\n",
    "model = Net2NetTransformer(**celebAHQ_config.model.params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#Load checkpoints\n",
    "ckpt_path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\configs\\faceshq.ckpt\"\n",
    "sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "model.load_state_dict(sd)\n",
    "missing, unexpected = model.load_state_dict(sd, strict=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch.autograd.grad_mode.set_grad_enabled at 0x1c8a5ace800>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Put model in evaluation mode\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#We look at the codebook to check whether or not it ressembles a normal distribution around 0.\n",
    "\n",
    "norms = []\n",
    "\n",
    "#Uncomment the code below to get a graph of how the freshly initialized codebook should look.\n",
    "#model.first_stage_model.quantize.embedding.weight.data.uniform_(-1.0/(model.first_stage_model.quantize.n_e),1/(model.first_stage_model.quantize.n_e))\n",
    "\n",
    "for codebook in model.first_stage_model.quantize.parameters():\n",
    "    codebook\n",
    "    for zq in codebook:\n",
    "        norms += [np.linalg.norm(zq)]\n",
    "\n",
    "#We know for a fact, that all norms will be equal to or less than 2 if it has just been instantiated\n",
    "assert(len([norm for norm in norms if norm>2])>0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load example data\n",
    "\n",
    "Load an example segmentation and visualize."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\DripTooHard\\\\PycharmProjects\\\\taming-transformers2\\\\data\\\\FFHQ\\\\00000\\\\00001.png'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 19\u001B[0m\n\u001B[0;32m     16\u001B[0m FFHQ_example_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mUsers\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mDripTooHard\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mPycharmProjects\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mtaming-transformers2\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mFFHQ\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m00000\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m00001.png\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     18\u001B[0m AHQ_image \u001B[38;5;241m=\u001B[39m load_image_tensor(AHQ_example_path)\n\u001B[1;32m---> 19\u001B[0m FFHQ_image \u001B[38;5;241m=\u001B[39m \u001B[43mload_image_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mFFHQ_example_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# Plot the images with titles\u001B[39;00m\n\u001B[0;32m     22\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m5\u001B[39m))\n",
      "Cell \u001B[1;32mIn[7], line 7\u001B[0m, in \u001B[0;36mload_image_tensor\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_image_tensor\u001B[39m(path):\n\u001B[1;32m----> 7\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m     transform \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[0;32m      9\u001B[0m         transforms\u001B[38;5;241m.\u001B[39mPILToTensor()\n\u001B[0;32m     10\u001B[0m     ])\n\u001B[0;32m     11\u001B[0m     img_tensor \u001B[38;5;241m=\u001B[39m transform(image)\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m255\u001B[39m\n",
      "File \u001B[1;32m~\\PycharmProjects\\taming-transformers2\\venv\\lib\\site-packages\\PIL\\Image.py:3243\u001B[0m, in \u001B[0;36mopen\u001B[1;34m(fp, mode, formats)\u001B[0m\n\u001B[0;32m   3240\u001B[0m     filename \u001B[38;5;241m=\u001B[39m fp\n\u001B[0;32m   3242\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m filename:\n\u001B[1;32m-> 3243\u001B[0m     fp \u001B[38;5;241m=\u001B[39m \u001B[43mbuiltins\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3244\u001B[0m     exclusive_fp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   3246\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\DripTooHard\\\\PycharmProjects\\\\taming-transformers2\\\\data\\\\FFHQ\\\\00000\\\\00001.png'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the function to load an image as a tensor\n",
    "def load_image_tensor(path):\n",
    "    image = Image.open(path)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.PILToTensor()\n",
    "    ])\n",
    "    img_tensor = transform(image).permute(1,2,0).float()/255\n",
    "    return img_tensor.permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "# Load the images\n",
    "AHQ_example_path = r\"C:\\Users\\DripTooHard\\Pictures\\DivVisaApp.jpg\"\n",
    "FFHQ_example_path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\data\\FFHQ\\00000\\00001.png\"\n",
    "\n",
    "AHQ_image = load_image_tensor(AHQ_example_path)\n",
    "FFHQ_image = load_image_tensor(FFHQ_example_path)\n",
    "\n",
    "# Plot the images with titles\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(AHQ_image[0].permute(1, 2, 0))\n",
    "plt.title('CelebAHQ')\n",
    "plt.axis('off')  # Hide the axis\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(FFHQ_image[0].permute(1, 2, 0))\n",
    "plt.title('FFHQ')\n",
    "plt.axis('off')  # Hide the axis\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 2: We now confirm, that it can correctly reconstruct the images.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def deconstruct_reconstruct(image):\n",
    "    image = image.type(torch.FloatTensor)  # Ensure the image tensor is of type FloatTensor\n",
    "    image_zq, image_z_indices = model.encode_to_z(image)\n",
    "    image_recon = model.decode_to_img(image_z_indices, image_zq.shape)\n",
    "    return image_recon\n",
    "\n",
    "model.set_epsilon(0.9)\n",
    "recon = deconstruct_reconstruct(AHQ_image)\n",
    "recon = recon"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the images with titles\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(recon[0].float().permute(1, 2, 0))\n",
    "plt.title('CelebAHQ')\n",
    "plt.axis('off')  # Hide the axis\n",
    "plt.savefig(\"TestImageReconstructed\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Step 3:\n",
    "\n",
    "We now add the ability to use the laplace mechanism when encoding the images.\n",
    "\n",
    "We now recreate the images with noise and plot them next to eachother.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "\n",
    "def deconstruct_reconstruct(image, epsilon):\n",
    "    image = image.type(torch.FloatTensor)  # Ensure the image tensor is of type FloatTensor\n",
    "    model.set_epsilon(epsilon)\n",
    "    image_zq, image_z_indices = model.encode_to_z(image)\n",
    "    image_recon = model.decode_to_img(image_z_indices, image_zq.shape)\n",
    "    return image_recon\n",
    "\n",
    "#Ignore this for now\n",
    "def deconstruct_reconstruct_mix(image1,image2,epsilon):\n",
    "    image1 = image1.type(torch.FloatTensor)  # Ensure the image tensor is of type FloatTensor\n",
    "    image2 = image2.type(torch.FloatTensor)  # Ensure the image tensor is of type FloatTensor\n",
    "    image_zq, image_z_indices = model.encode_to_z_mix(image1,image2, epsilon)\n",
    "    image_recon = model.decode_to_img(image_z_indices, image_zq.shape)\n",
    "    return image_recon\n",
    "\n",
    "\n",
    "step_size = 0.01\n",
    "max_range = 1\n",
    "\n",
    "epsilon_values = np.arange(0,max_range,step_size)\n",
    "\n",
    "folder = fr\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\ExperimentResults\\NoisyReconstructions\\Laplace{max_range}{step_size}\"\n",
    "try:\n",
    "    os.makedirs(folder)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if(doGradualEpsilonXperiment):\n",
    "    for epsilon in epsilon_values:\n",
    "        # Assuming AHQ_image and FFHQ_image are defined and are tensors\n",
    "        AHQ_recon = deconstruct_reconstruct(AHQ_image, epsilon)\n",
    "        FFHQ_recon = deconstruct_reconstruct(FFHQ_image, epsilon)\n",
    "\n",
    "        # Plot the images with titles\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(AHQ_recon[0].permute(1, 2, 0))\n",
    "        plt.title('CelebAHQ-Reconstructed')\n",
    "        plt.axis('off')  # Hide the axis\n",
    "\n",
    "        # Use numpy to clip the values and ensure the correct data type for imsave\n",
    "\n",
    "        plt.imsave(f\"{folder}\\AHQ\\{epsilon}CelebAHQ-Reconstructed.png\", AHQ_recon[0].permute(1, 2, 0).numpy().clip(0, 1))\n",
    "        plt.close()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(FFHQ_recon[0].permute(1, 2, 0))\n",
    "        plt.title('FFHQ-reconstructed')\n",
    "        plt.axis('off')  # Hide the axis\n",
    "\n",
    "        # Use numpy to clip the values and ensure the correct data type for imsave\n",
    "        plt.imsave(f\"{folder}\\FFHQ\\{epsilon}FFHQ-reconstructed.png\", FFHQ_recon[0].permute(1, 2, 0).numpy().clip(0, 1))\n",
    "        plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Step 4 Dataloader Object\n",
    "\n",
    "The point is now to create an object, which either:\n",
    "\n",
    "1) takes a 3-tuple of percentages and a .txt file of image paths and seperates the images paths into three different .txt files, one for validation, one for testing and a third for training.\n",
    "\n",
    "2) A tuple with (valid_paths.txt,test_paths.txt,train_paths.txt)\n",
    "\n",
    "We would then like for the dataloader object to be able to divide the image paths into minibatches, which we should be able to get by using .getMinibatch(i) = ith minibatch. It should have a parameter, which based on the size of the minibatches, tells us how many minibatches there are per epoch.\n",
    "\n",
    "Finally, we would like for it to be able to turn each minibatch of paths into a minibatch of images with getMinibatchImages(i).\n",
    "\n",
    "The dimension of the images should be specified beforehand, and should match the dimensions of our VQGAN model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}