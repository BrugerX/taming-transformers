{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Goal of this notebook\n",
    "\n",
    "We would like to confirm whether or not there is a relation between geometric closeness in the latent space and semantic similarity.\n",
    "\n",
    "Specifically; For images with encoded and/or quantized representation that are close to each other, are the images themselves semantically close too?\n",
    "\n",
    "This assumption underpins the whole reasoning behind our utilization of differential privacy. And we would therefore like to investigate it.\n",
    "\n",
    "0) Load the model\n",
    "1) For M images get the quantized, encoded, original and reconstruction of images as well as their paths.\n",
    "2) For the images get N nearest neighbours for their encodings as well as quantizations. Store this in the same row in the formats \"Nearest neighbours quant/latent\": [{\"name\":,\"path\":,\"closeness\"}]. Do it this way, so we can retreive the images quickly, but also, so we can compare HOW close the images are with their perceptual likeness\n",
    "\n",
    "3) For each of these clusters investigate the perceptual reconstruction loss between their original images and store this as an array in the same row in the format:\n",
    " \"Nearest neighbours quant/latent\": [{\"name\":,\"path\":,\"closeness\"}]\n",
    "\n",
    "Once we have done it on a smaller scale on my PC, we should run the same project but on the HPC. So keep this in mind."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from dataloader.gdrive.GDriveHandler import GDrive_Handler\n",
    "import numpy as np\n",
    "import pytorch_lightning.core as L\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import dataloader.GDriveDataloading as GDTL\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image,ImageShow\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from omegaconf import OmegaConf\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import main\n",
    "import taming.modules.losses.vqperceptual\n",
    "from taming.models.vqgan import LAPVQ\n",
    "from taming.models.cond_transformer import Net2NetTransformer\n",
    "from scipy.spatial.distance import euclidean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT ID: 146\n"
     ]
    }
   ],
   "source": [
    "#Generate an ID to identify this experiment\n",
    "random_generator = random.Random()\n",
    "#DF_id = random_generator.randint(0,10000)\n",
    "DF_id = 146\n",
    "print(f\"EXPERIMENT ID: {DF_id}\")\n",
    "\n",
    "#We create a folder to host the experiment in:\n",
    "xperiment_folder_dir = f\"{DF_id}SemanticsExperiment\\\\\"\n",
    "if not os.path.exists(xperiment_folder_dir):\n",
    "    os.makedirs(xperiment_folder_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "  params:\n",
      "    batch_size: 2\n",
      "    num_workers: 8\n",
      "    train:\n",
      "      params:\n",
      "        coord: true\n",
      "        crop_size: 256\n",
      "        size: 256\n",
      "      target: taming.data.faceshq.FacesHQTrain\n",
      "    validation:\n",
      "      params:\n",
      "        coord: true\n",
      "        crop_size: 256\n",
      "        size: 256\n",
      "      target: taming.data.faceshq.FacesHQValidation\n",
      "  target: main.DataModuleFromConfig\n",
      "model:\n",
      "  base_learning_rate: 4.5e-06\n",
      "  params:\n",
      "    cond_stage_config:\n",
      "      params:\n",
      "        down_factor: 16\n",
      "        n_embed: 1024\n",
      "      target: taming.modules.misc.coord.CoordStage\n",
      "    cond_stage_key: coord\n",
      "    first_stage_config:\n",
      "      params:\n",
      "        ckpt_path: C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\configs\\faceshq.ckpt\n",
      "        ddconfig:\n",
      "          attn_resolutions:\n",
      "          - 16\n",
      "          ch: 128\n",
      "          ch_mult:\n",
      "          - 1\n",
      "          - 1\n",
      "          - 2\n",
      "          - 2\n",
      "          - 4\n",
      "          double_z: false\n",
      "          dropout: 0.0\n",
      "          in_channels: 3\n",
      "          num_res_blocks: 2\n",
      "          out_ch: 3\n",
      "          resolution: 256\n",
      "          z_channels: 256\n",
      "        embed_dim: 256\n",
      "        lossconfig:\n",
      "          target: taming.modules.losses.DummyLoss\n",
      "        n_embed: 1024\n",
      "      target: taming.models.vqgan.LAPVQ\n",
      "    transformer_config:\n",
      "      params:\n",
      "        block_size: 512\n",
      "        n_embd: 1024\n",
      "        n_head: 16\n",
      "        n_layer: 24\n",
      "        vocab_size: 1024\n",
      "      target: taming.modules.transformer.mingpt.GPT\n",
      "  target: taming.models.cond_transformer.Net2NetTransformer\n",
      "\n",
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<torch.autograd.grad_mode.set_grad_enabled at 0x21b8ecbd540>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare CelebAHQ configurations\n",
    "config_path = fr\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\configs\\faceshq_transformer.yaml\"\n",
    "celebAHQ_config = OmegaConf.load(config_path)\n",
    "print(yaml.dump(OmegaConf.to_container(celebAHQ_config)))\n",
    "\n",
    "# Init model with the chosen architecture and configurations\n",
    "model = Net2NetTransformer(**celebAHQ_config.model.params)\n",
    "\n",
    "#Load checkpoints\n",
    "ckpt_path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\configs\\faceshq.ckpt\"\n",
    "sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "model.load_state_dict(sd)\n",
    "missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "\n",
    "#Put model in evaluation mode\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#Name of DF with image paths and names\n",
    "image_paths_DF_path = \"image_paths_DF.csv\"\n",
    "\n",
    "#The directory to the images\n",
    "try:\n",
    "    image_paths_DF = pd.read_csv(f\"{xperiment_folder_dir}{image_paths_DF_path}\")\n",
    "except:\n",
    "    data_dir_path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\data\\00000-20231203T065959Z-001\\00000\"\n",
    "\n",
    "    data_directory = os.fsencode(data_dir_path)\n",
    "\n",
    "    images = []\n",
    "\n",
    "    for file in os.listdir(data_directory):\n",
    "         filename = os.fsdecode(file)\n",
    "         images += [{\"filename\":filename,\"path\":f\"{data_dir_path}\\\\{filename}\"}]\n",
    "\n",
    "    image_paths_DF = pd.DataFrame.from_dict(images)\n",
    "    image_paths_DF.to_csv(f\"{xperiment_folder_dir}{image_paths_DF_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_image_VQGAN(image_path):\n",
    "     image_list = []\n",
    "     for filename in glob.glob(image_path):\n",
    "         im=Image.open(filename)\n",
    "\n",
    "     transform = transforms.Compose([\n",
    "     transforms.PILToTensor() ])\n",
    "\n",
    "     #Our model takes floats in the range of [0,1]\n",
    "     im = (transform(im).float()/255)\n",
    "\n",
    "     return im.unsqueeze(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def representation_worker(paths_list):\n",
    "     image_array = []\n",
    "\n",
    "     for image_path in paths_list:\n",
    "          image_tensor = load_image_VQGAN(image_path)\n",
    "          latent_representation = model.first_stage_model.encoder(image_tensor)\n",
    "          quantized_representation = model.first_stage_model.quant_conv(latent_representation)\n",
    "          quantized_representation,_,_ = model.first_stage_model.quantize(quantized_representation)\n",
    "          reconstruction = model.first_stage_model.decode(quantized_representation)\n",
    "          image_array += [{\"image_path\":image_path,\"image_original\":image_tensor,\"latent_representation\":latent_representation,\"quantized_representation\":quantized_representation,\"image_reconstruction\":reconstruction}]\n",
    "\n",
    "     return image_array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import threading\n",
    "import queue\n",
    "\n",
    "def divide_paths_for_workers(num_workers, image_paths_list):\n",
    "    # Calculate the number of paths per worker\n",
    "    total_paths = len(image_paths_list)\n",
    "    paths_per_worker = total_paths // num_workers\n",
    "    remainder = total_paths % num_workers\n",
    "\n",
    "    paths_per_thread = []\n",
    "    start = 0\n",
    "\n",
    "    # Distribute the paths among the workers\n",
    "    for i in range(num_workers):\n",
    "        # Add an extra path to some workers to distribute the remainder\n",
    "        end = start + paths_per_worker + (1 if i < remainder else 0)\n",
    "        paths_per_thread.append(image_paths_list[start:end])\n",
    "        start = end\n",
    "\n",
    "    return paths_per_thread\n",
    "\"\"\"\n",
    "This is the parallel version\n",
    "\n",
    "def representation_worker(paths_list, result_queue):\n",
    "    image_array = []\n",
    "\n",
    "    for image_path in paths_list:\n",
    "        image_tensor = load_image_VQGAN(image_path)\n",
    "        latent_representation = model.first_stage_model.encoder(image_tensor)\n",
    "        quantized_representation = model.first_stage_model.quant_conv(latent_representation)\n",
    "        quantized_representation, _, _ = model.first_stage_model.quantize(quantized_representation)\n",
    "        reconstruction = model.first_stage_model.decode(quantized_representation)\n",
    "        image_array.append({\"image_path\": image_path, \"image_original\": image_tensor,\n",
    "                            \"latent_representation\": latent_representation,\n",
    "                            \"quantized_representation\": quantized_representation,\n",
    "                            \"image_reconstruction\": reconstruction})\n",
    "\n",
    "    result_queue.put(image_array)\n",
    "\"\"\"\n",
    "def worker(paths, result_queue):\n",
    "    representation_worker(paths, result_queue)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#Name of DF with the different representations\n",
    "representations_DF_path = \"representation_DF.csv\"\n",
    "try:\n",
    "    representations_DF = pd.read_csv(f\"{xperiment_folder_dir}{representations_DF_path}\")\n",
    "except:\n",
    "    M = 20\n",
    "    num_workers = 1 #Number of cores used\n",
    "\n",
    "\n",
    "    image_arrays = []\n",
    "    image_paths_list = image_paths_DF[\"path\"].iloc[0:M].tolist()\n",
    "\n",
    "    \"\"\"\n",
    "    result_queue = queue.Queue()\n",
    "    threads = []\n",
    "    paths_per_thread = divide_paths_for_workers(num_workers, image_paths_list)\n",
    "\n",
    "\n",
    "    # Create and start threads\n",
    "    for paths in paths_per_thread:\n",
    "        thread = threading.Thread(target=worker, args=(paths, result_queue))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    # Wait for all threads to finish\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    # Aggregate results\n",
    "    all_results = []\n",
    "    while not result_queue.empty():\n",
    "        all_results.extend(result_queue.get())\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    representation_DF =  pd.DataFrame.from_dict(representation_worker(image_paths_list))\n",
    "    representation_DF.to_csv(f\"{DF_id}{representations_DF_path}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def flatten_latent_representation(latent_rep):\n",
    "    return latent_rep.reshape(-1, latent_rep.shape[1])\n",
    "\n",
    "results = []\n",
    "\n",
    "# Iterate over each combination of rows\n",
    "for i in range(len(representation_DF)):\n",
    "    for j in range(i+1, len(representation_DF)):  # Start from i+1 to avoid duplicate pairs and comparing with itself\n",
    "        row_i = representation_DF.iloc[i]\n",
    "        row_j = representation_DF.iloc[j]\n",
    "\n",
    "        # Flatten the latent representations\n",
    "        flat_i = flatten_latent_representation(row_i['latent_representation'])\n",
    "        flat_j = flatten_latent_representation(row_j['latent_representation'])\n",
    "\n",
    "        # Ensure both have the same number of vectors\n",
    "        if flat_i.shape[0] != flat_j.shape[0]:\n",
    "            raise ValueError(\"Mismatch in the number of vectors in the latent representations\")\n",
    "\n",
    "        # Calculate distances between corresponding vectors\n",
    "        for vec_index in range(flat_i.shape[0]):\n",
    "            vec_i = flat_i[vec_index]\n",
    "            vec_j = flat_j[vec_index]\n",
    "            distance = euclidean(vec_i, vec_j)\n",
    "\n",
    "            # Append the result as a dictionary\n",
    "            results.append({\n",
    "                \"image_path_1\": row_i['image_path'],\n",
    "                \"latent_representation_1\": vec_i,\n",
    "                \"image_path_2\": row_j['image_path'],\n",
    "                \"latent_representation_2\": vec_j,\n",
    "                \"distance\": distance\n",
    "            })\n",
    "\n",
    "\n",
    "latent_representation_DF = pd.DataFrame.from_dict(results)\n",
    "latent_representation_DF.to_csv(f\"{DF_id}latent_representation_DF.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "latent_representation_DF.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}