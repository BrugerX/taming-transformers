{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gdrive.GDriveHandler as GDH\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning.core as L\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import GDriveDataloading as GDTL\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n",
      "Download 100%.\n"
     ]
    }
   ],
   "source": [
    "#data_path,scopes,creds_path,write_new_token = True,GDrive_token_path = \"token.json\",batch_size: int = 32\n",
    "\n",
    "ID_DF_Path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\scripts\\dataloader\\gdrive\\FFHQimages.csv\"\n",
    "config_path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\configs\\gdrive_FFHQ.yaml\"\n",
    "scopes = [\"https://www.googleapis.com/auth/drive.readonly\"]\n",
    "creds_path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\scripts\\dataloader\\gdrive\\deep-learning-2023-405822-135193813109.json\"\n",
    "access_token_path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\scripts\\dataloader\\test.json\"\n",
    "\n",
    "GDDataloader = GDTL.ImagesDatamodule(ID_DF_Path,scopes,creds_path)\n",
    "prop_dict = dict({\"val\":0.1,\"test\":0.1,\"train\":0.8})\n",
    "GDDataloader.setup(prop_dict)\n",
    "\n",
    "train_dataloader = GDDataloader.train_dataloader()\n",
    "\n",
    "#We get a single batch, so I don't have to download it all the time\n",
    "example_batch = next(iter(train_dataloader))\n",
    "example_batch = example_batch.float()/255"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## We will now test to see if the dataloader works correctly with the pre-existing VQGAN object"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from PIL import Image,ImageShow\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from omegaconf import OmegaConf\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import main\n",
    "import taming.modules.losses.vqperceptual\n",
    "from taming.models.cond_transformer import Net2NetTransformer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "  params:\n",
      "    batch_size: 2\n",
      "    num_workers: 8\n",
      "    train:\n",
      "      params:\n",
      "        coord: true\n",
      "        crop_size: 256\n",
      "        size: 256\n",
      "      target: taming.data.faceshq.FacesHQTrain\n",
      "    validation:\n",
      "      params:\n",
      "        coord: true\n",
      "        crop_size: 256\n",
      "        size: 256\n",
      "      target: taming.data.faceshq.FacesHQValidation\n",
      "  target: main.DataModuleFromConfig\n",
      "model:\n",
      "  base_learning_rate: 4.5e-06\n",
      "  params:\n",
      "    cond_stage_config:\n",
      "      params:\n",
      "        down_factor: 16\n",
      "        n_embed: 1024\n",
      "      target: taming.modules.misc.coord.CoordStage\n",
      "    cond_stage_key: coord\n",
      "    first_stage_config:\n",
      "      params:\n",
      "        ckpt_path: C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\configs\\faceshq.ckpt\n",
      "        ddconfig:\n",
      "          attn_resolutions:\n",
      "          - 16\n",
      "          ch: 128\n",
      "          ch_mult:\n",
      "          - 1\n",
      "          - 1\n",
      "          - 2\n",
      "          - 2\n",
      "          - 4\n",
      "          double_z: false\n",
      "          dropout: 0.0\n",
      "          in_channels: 3\n",
      "          num_res_blocks: 2\n",
      "          out_ch: 3\n",
      "          resolution: 256\n",
      "          z_channels: 256\n",
      "        embed_dim: 256\n",
      "        lossconfig:\n",
      "          target: taming.modules.losses.DummyLoss\n",
      "        n_embed: 1024\n",
      "      target: taming.models.vqgan.LAPVQ\n",
      "    transformer_config:\n",
      "      params:\n",
      "        block_size: 512\n",
      "        n_embd: 1024\n",
      "        n_head: 16\n",
      "        n_layer: 24\n",
      "        vocab_size: 1024\n",
      "      target: taming.modules.transformer.mingpt.GPT\n",
      "  target: taming.models.cond_transformer.Net2NetTransformer\n",
      "\n",
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n"
     ]
    }
   ],
   "source": [
    "#Prepare CelebAHQ configurations\n",
    "config_path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\configs\\faceshq_transformer.yaml\"\n",
    "celebAHQ_config = OmegaConf.load(config_path)\n",
    "print(yaml.dump(OmegaConf.to_container(celebAHQ_config)))\n",
    "\n",
    "#Init model with the chosen architecture and configurations\n",
    "model = Net2NetTransformer(**celebAHQ_config.model.params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.training_step({\"image\":example_batch,\"coord\":np.zeros(32)},1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[218, 221, 226],\n         [218, 221, 226],\n         [218, 221, 226],\n         ...,\n         [147, 147, 153],\n         [144, 145, 150],\n         [143, 144, 149]],\n\n        [[218, 221, 226],\n         [218, 221, 226],\n         [218, 221, 227],\n         ...,\n         [147, 148, 153],\n         [145, 146, 151],\n         [144, 145, 150]],\n\n        [[216, 221, 226],\n         [216, 221, 227],\n         [217, 222, 229],\n         ...,\n         [145, 146, 151],\n         [145, 146, 151],\n         [145, 146, 151]],\n\n        ...,\n\n        [[243, 221, 197],\n         [242, 219, 193],\n         [239, 214, 188],\n         ...,\n         [112, 104,  83],\n         [112, 105,  84],\n         [111, 104,  83]],\n\n        [[243, 222, 196],\n         [240, 218, 191],\n         [237, 213, 186],\n         ...,\n         [112, 102,  82],\n         [112, 103,  82],\n         [112, 104,  83]],\n\n        [[244, 223, 196],\n         [239, 218, 191],\n         [237, 213, 185],\n         ...,\n         [114, 103,  82],\n         [113, 102,  82],\n         [114, 103,  83]]], dtype=torch.uint8)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GDDataloader.teardown(None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}