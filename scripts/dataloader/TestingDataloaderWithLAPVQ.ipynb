{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "  params:\n",
      "    batch_size: 3\n",
      "    num_workers: 8\n",
      "    train:\n",
      "      params:\n",
      "        crop_size: 256\n",
      "        size: 256\n",
      "      target: taming.data.faceshq.FacesHQTrain\n",
      "    validation:\n",
      "      params:\n",
      "        crop_size: 256\n",
      "        size: 256\n",
      "      target: taming.data.faceshq.FacesHQValidation\n",
      "  target: main.DataModuleFromConfig\n",
      "model:\n",
      "  base_learning_rate: 4.5e-06\n",
      "  params:\n",
      "    ddconfig:\n",
      "      attn_resolutions:\n",
      "      - 16\n",
      "      ch: 128\n",
      "      ch_mult:\n",
      "      - 1\n",
      "      - 1\n",
      "      - 2\n",
      "      - 2\n",
      "      - 4\n",
      "      double_z: false\n",
      "      dropout: 0.0\n",
      "      in_channels: 3\n",
      "      num_res_blocks: 2\n",
      "      out_ch: 3\n",
      "      resolution: 256\n",
      "      z_channels: 256\n",
      "    embed_dim: 256\n",
      "    lossconfig:\n",
      "      params:\n",
      "        codebook_weight: 1.0\n",
      "        disc_conditional: false\n",
      "        disc_in_channels: 3\n",
      "        disc_start: 30001\n",
      "        disc_weight: 0.8\n",
      "      target: taming.modules.losses.vqperceptual.VQLPIPSWithDiscriminator\n",
      "    n_embed: 1024\n",
      "  target: taming.models.vqgan.LAPVQ\n",
      "\n",
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips\\vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n"
     ]
    }
   ],
   "source": [
    "import gdrive.GDriveHandler as GDH\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning.core as L\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import GDriveDataloading as GDTL\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image,ImageShow\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from omegaconf import OmegaConf\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import main\n",
    "import taming.modules.losses.vqperceptual\n",
    "from taming.models.cond_transformer import Net2NetTransformer\n",
    "from taming.models.vqgan import  LAPVQ\n",
    "\n",
    "\n",
    "\n",
    "ID_DF_Path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\scripts\\dataloader\\gdrive\\FFHQimages.csv\"\n",
    "config_path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\configs\\gdrive_FFHQ.yaml\"\n",
    "scopes = [\"https://www.googleapis.com/auth/drive.readonly\"]\n",
    "creds_path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\scripts\\dataloader\\gdrive\\deep-learning-2023-405822-135193813109.json\"\n",
    "access_token_path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\scripts\\dataloader\\test.json\"\n",
    "\n",
    "GDDataloader = GDTL.ImagesDatamodule(ID_DF_Path,scopes,creds_path,num_workers=0,batch_size=3)\n",
    "prop_dict = dict({\"val\":0.1,\"test\":0.1,\"train\":0.8})\n",
    "GDDataloader.setup(prop_dict)\n",
    "\n",
    "train_dataloader = GDDataloader.train_dataloader()\n",
    "start = time.time()\n",
    "iter_train = iter(train_dataloader)\n",
    "\n",
    "#Prepare CelebAHQ configurations\n",
    "config_path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers2\\configs\\faceshq_vqgan.yaml\"\n",
    "celebAHQ_config = OmegaConf.load(config_path)\n",
    "print(yaml.dump(OmegaConf.to_container(celebAHQ_config)))\n",
    "\n",
    "#Init model with the chosen architecture and configurations\n",
    "model = LAPVQ(**celebAHQ_config.model.params)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model.set_epsilon(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}